{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ajitesh Lab 10.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrRw9PHzhZ_0",
        "colab_type": "text"
      },
      "source": [
        " **Problem Statetemnt 1**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IgJpUYxgYWK",
        "colab_type": "code",
        "colab": {},
        "cellView": "code"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "import numpy as np\n",
        "#from sklearn import preprocessing, cross_validation\n",
        "import pandas as pd\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmKO4cVDiKgD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "np.random.seed(123)\n",
        "\n",
        "allwalks = []\n",
        "\n",
        "for i in range(250):\n",
        "    randwalk = [0]\n",
        "    for x in range(100):\n",
        "        step = randwalk[-1]\n",
        "        dice = np.random.randint(1,7)\n",
        "        if dice <= 2 :\n",
        "            step = max(0, step - 1)\n",
        "\n",
        "        elif dice<=5:\n",
        "            step += 1\n",
        "\n",
        "        else:\n",
        "            step = step + np.random.randint(1,7)\n",
        "        \n",
        "    print(step)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBaCAZS7h0MW",
        "colab_type": "text"
      },
      "source": [
        "**Problem Statement 2**\n",
        "\n",
        "**Random data for multiple linear regression**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-tcSNKMiIB_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "outputId": "5df495a8-40d6-47cf-8d36-6b90cbb0b7ac"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy\n",
        "import random\n",
        "from scipy.stats import norm\n",
        "random.seed(1)\n",
        "n_features = 4\n",
        "X = []\n",
        "for i in range(n_features):\n",
        "  X_i = scipy.stats.norm.rvs(0, 1, 100)\n",
        "  X.append(X_i)\n",
        "#print(X)\n",
        "eps = scipy.stats.norm.rvs(0, 0.25,100)\n",
        "y = 1 + (0.4 * X[0]) + eps + (0.5 * X[1]) + (0.3 * X[2]) + (0.4 * X[3])\n",
        "data_mlr = {'X0': X[0],'X1':X[1],'X2':X[2],'X3':X[3],'Y': y }\n",
        "df = pd.DataFrame(data_mlr)\n",
        "print(df.head())\n",
        "print(df.tail())\n",
        "print(df.info())\n",
        "print(df.describe())\n",
        "#df.to_csv('file1.csv')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "         X0        X1        X2        X3         Y\n",
            "0  0.102660 -0.675052  0.479516 -0.671916  0.651795\n",
            "1 -1.583859  0.447038 -1.213997 -0.577359 -0.056761\n",
            "2  1.882197 -0.569644  0.336631 -0.891719  1.082657\n",
            "3 -0.340632 -0.088431  1.010624  0.039143  0.998387\n",
            "4  0.389093 -1.240797  1.272859 -0.946486  0.121126\n",
            "          X0        X1        X2        X3         Y\n",
            "95  0.572965 -0.806952 -1.168995  0.525856  0.461042\n",
            "96  0.836125  0.258976 -0.515234 -0.317285  1.095143\n",
            "97  0.120351 -0.673208 -0.042428  0.714133  1.066757\n",
            "98  1.079557 -0.669846 -0.401565  0.852115  1.317225\n",
            "99  2.416417  0.257020 -0.262359 -0.275228  1.517372\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100 entries, 0 to 99\n",
            "Data columns (total 5 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   X0      100 non-null    float64\n",
            " 1   X1      100 non-null    float64\n",
            " 2   X2      100 non-null    float64\n",
            " 3   X3      100 non-null    float64\n",
            " 4   Y       100 non-null    float64\n",
            "dtypes: float64(5)\n",
            "memory usage: 4.0 KB\n",
            "None\n",
            "               X0          X1          X2          X3           Y\n",
            "count  100.000000  100.000000  100.000000  100.000000  100.000000\n",
            "mean    -0.002879   -0.059077    0.069491   -0.173199    0.884866\n",
            "std      1.015543    0.972043    1.108894    1.007179    0.872783\n",
            "min     -2.713198   -2.765529   -3.180223   -3.604642   -1.702967\n",
            "25%     -0.754416   -0.688046   -0.638851   -0.704303    0.323284\n",
            "50%     -0.046563   -0.060133    0.086903   -0.217212    1.026395\n",
            "75%      0.754313    0.683804    0.707443    0.436300    1.394051\n",
            "max      2.416417    1.944845    2.815913    2.158094    3.446000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NXuH7LVidS8",
        "colab_type": "text"
      },
      "source": [
        "**Random data for logistic regression**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ecYgMVKikOx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "outputId": "b0ea5de2-614a-48e1-e1e3-29d8713a719d"
      },
      "source": [
        "n_features = 4\n",
        "X = []\n",
        "for i in range(n_features):\n",
        "  X_i = scipy.stats.norm.rvs(0, 1, 100)\n",
        "  X.append(X_i)\n",
        "#print(X)\n",
        "a1 = (np.exp(1 + (0.5 * X[0]) + (0.4 * X[1]) + (0.3 * X[2]) + (0.5 * X[3]))/(1 + np.exp(1 + (0.5 * X[0]) + (0.4 * X[1]) + (0.3 * X[2]) + (0.5 * X[3]))))\n",
        "#print(a1)\n",
        "y1 = []\n",
        "for i in a1:\n",
        "  if (i>=0.5):\n",
        "    y1.append(1)\n",
        "  else:\n",
        "    y1.append(0)\n",
        "#print(y1)\n",
        "data_lr = {'X0': X[0],'X1':X[1],'X2':X[2],'X3':X[3],'Y': y1 }\n",
        "df1 = pd.DataFrame(data_lr)\n",
        "print(df1.head())\n",
        "print(df1.tail())\n",
        "print(df1.info())\n",
        "print(df1.describe())\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "         X0        X1        X2        X3  Y\n",
            "0 -0.315586 -0.258934 -0.905684 -0.438915  1\n",
            "1  1.294723 -0.268586  0.030315 -0.756981  1\n",
            "2  1.691310  0.021330 -1.132960  1.049527  1\n",
            "3  2.001563  0.268078  0.879611  0.348544  1\n",
            "4 -0.351929 -0.153433 -0.976083  1.556172  1\n",
            "          X0        X1        X2        X3  Y\n",
            "95 -0.996109 -0.080439  0.210552 -3.073589  0\n",
            "96  0.960030  0.932082 -0.081797 -0.281554  1\n",
            "97 -0.803831 -1.132952  0.114917 -1.172215  0\n",
            "98 -0.184631 -0.427502 -0.369923  1.464738  1\n",
            "99  0.424561  0.504648  0.704489 -1.734150  1\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100 entries, 0 to 99\n",
            "Data columns (total 5 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   X0      100 non-null    float64\n",
            " 1   X1      100 non-null    float64\n",
            " 2   X2      100 non-null    float64\n",
            " 3   X3      100 non-null    float64\n",
            " 4   Y       100 non-null    int64  \n",
            "dtypes: float64(4), int64(1)\n",
            "memory usage: 4.0 KB\n",
            "None\n",
            "               X0          X1          X2          X3           Y\n",
            "count  100.000000  100.000000  100.000000  100.000000  100.000000\n",
            "mean     0.072122    0.018817   -0.071034   -0.203137    0.820000\n",
            "std      0.961681    1.001330    0.916374    1.047511    0.386123\n",
            "min     -2.126456   -2.268576   -2.709839   -3.073589    0.000000\n",
            "25%     -0.452225   -0.738901   -0.641009   -0.787571    1.000000\n",
            "50%      0.132619    0.011410   -0.049437   -0.231628    1.000000\n",
            "75%      0.782006    0.678298    0.580278    0.553371    1.000000\n",
            "max      2.044744    2.394512    2.007709    2.979047    1.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giUI7zkCisSH",
        "colab_type": "text"
      },
      "source": [
        "**Random data for K means clustering** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwiQhBpYizzS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 792
        },
        "outputId": "5861e2ab-2377-4d3d-ae7d-0fef8b10048c"
      },
      "source": [
        "X_a= -2 * np.random.rand(100,2)\n",
        "X_b = 1 + 2 * np.random.rand(50,2)\n",
        "X_a[50:100, :] = X_b\n",
        "plt.scatter(X_a[ : , 0], X_a[ :, 1], s = 50)\n",
        "plt.show()\n",
        "data_kmeans = {'X0': X_a[:,0],'X1':X_a[:,1]}\n",
        "df3 = pd.DataFrame(data_kmeans)\n",
        "print(df3.head())\n",
        "print(df3.tail())\n",
        "print(df3.info())\n",
        "print(df3.describe())"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "         X0        X1\n",
            "0 -1.281356 -0.032758\n",
            "1 -0.139606 -0.896728\n",
            "2 -0.276449 -0.633706\n",
            "3 -0.692803 -0.545753\n",
            "4 -0.911630 -0.007666\n",
            "          X0        X1\n",
            "95  2.114635  1.494971\n",
            "96  2.235574  2.948637\n",
            "97  1.403506  2.388047\n",
            "98  2.486495  2.737578\n",
            "99  1.509016  2.180972\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100 entries, 0 to 99\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   X0      100 non-null    float64\n",
            " 1   X1      100 non-null    float64\n",
            "dtypes: float64(2)\n",
            "memory usage: 1.7 KB\n",
            "None\n",
            "               X0          X1\n",
            "count  100.000000  100.000000\n",
            "mean     0.593453    0.579179\n",
            "std      1.626362    1.582837\n",
            "min     -1.968086   -1.980840\n",
            "25%     -0.895887   -0.822365\n",
            "50%      0.569032    0.501803\n",
            "75%      2.122500    2.103657\n",
            "max      2.992586    2.997435\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skbBHOKCi7Iy",
        "colab_type": "text"
      },
      "source": [
        "**Problem statement 3**\n",
        "\n",
        "\n",
        "**Linear Regression using gradient descent**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DE3i6PnjBqd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9c63be5a-90be-48a4-8ac1-0f3f04bdb806"
      },
      "source": [
        "X = df.iloc[:,0].values\n",
        "#print(X)\n",
        "y = df.iloc[:,4].values\n",
        "b1 = 0\n",
        "b0 = 0\n",
        "l = 0.001\n",
        "epochs = 100\n",
        " \n",
        "n = float(len(X))\n",
        "for i in range(epochs):\n",
        "  y_p = b1*X + b0\n",
        "  loss = np.sum(y_p - y1)**2\n",
        "  d1 = (-2/n) * sum(X * (y - y_p))\n",
        "  d0 = (-2/n) * sum(y - y_p)\n",
        "  b1 = b1 - (l*d1)\n",
        "  b0 = b0 - (l*d0)\n",
        "\n",
        "print(b1,b0)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.08717733458871467 0.16056820208389166\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToXRvqTNjP7P",
        "colab_type": "text"
      },
      "source": [
        "**Logistic Regression using Gradient Descent**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ARKhx4pjWxd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "ea40232b-5ebb-4c30-b5d7-198d0fdc9bbd"
      },
      "source": [
        "X1 = df1.iloc[:,0:4].values\n",
        "y1 = df1.iloc[:,4].values\n",
        "\n",
        "def sigmoid(Z):\n",
        "  return 1 /(1+np.exp(-Z))\n",
        "\n",
        "def loss(y1,y_hat):\n",
        "  return -np.mean(y1*np.log(y_hat) + (1-y1)*(np.log(1-y_hat)))\n",
        "\n",
        "W = np.zeros((4,1))\n",
        "b = np.zeros((1,1))\n",
        "\n",
        "m = len(y1)\n",
        "lr = 0.001\n",
        "for epoch in range(1000):\n",
        "  Z = np.matmul(X1,W)+b\n",
        "  A = sigmoid(Z)\n",
        "  logistic_loss = loss(y1,A)\n",
        "  dz = A - y1\n",
        "  dw = 1/m * np.matmul(X1.T,dz)\n",
        "  db = np.sum(dz)\n",
        "\n",
        "  W = W - lr*dw\n",
        "  b = b - lr*db\n",
        "\n",
        "  if epoch % 100 == 0:\n",
        "    print(logistic_loss)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6931471805599453\n",
            "0.47063928947603983\n",
            "0.4699075146315108\n",
            "0.4691975865338838\n",
            "0.46850894063532633\n",
            "0.4678410224774447\n",
            "0.4671932876264788\n",
            "0.46656520160701165\n",
            "0.4659562398364653\n",
            "0.4653658875619398\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmqYLQN9jh4H",
        "colab_type": "text"
      },
      "source": [
        "**Linear Regreesion using L1 Regularization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11IrSphuji5N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4943f63b-487c-40ba-a206-3d2e84cc4106"
      },
      "source": [
        "X = df.iloc[:,0].values\n",
        "#print(X)\n",
        "y = df.iloc[:,4].values\n",
        "b1 = 0\n",
        "b0 = 0\n",
        "l = 0.001\n",
        "epochs = 100\n",
        "lam = 0.1\n",
        " \n",
        "n = float(len(X))\n",
        "for i in range(epochs):\n",
        "  y_p = b1*X + b0\n",
        "  loss = np.sum(y_p - y1)**2 + (lam * b1)\n",
        "  d1 = (-2/n) * sum(X * (y - y_p)) + lam\n",
        "  d0 = (-2/n) * sum(y - y_p)\n",
        "  b1 = b1 - (l*d1)\n",
        "  b0 = b0 - (l*d0)\n",
        "\n",
        "print(b1,b0)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.07812393063009228 0.16056570161007155\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVycRWD3jnpC",
        "colab_type": "text"
      },
      "source": [
        "**Linear Regreesion using L2 Regularization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pC-3Dfd8jodb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9258e027-134f-4252-a50e-73fc50d33b13"
      },
      "source": [
        "X = df.iloc[:,0].values\n",
        "#print(X)\n",
        "y = df.iloc[:,4].values\n",
        "b1 = 0\n",
        "b0 = 0\n",
        "l = 0.001\n",
        "epochs = 100\n",
        "lam = 0.1\n",
        " \n",
        "n = float(len(X))\n",
        "for i in range(epochs):\n",
        "  y_p = b1*X + b0\n",
        "  loss = np.sum(y_p - y1)**2 + ((lam/2) * b1)\n",
        "  d1 = (-2/n) * sum(X * (y - y_p)) + (lam *b1)\n",
        "  d0 = (-2/n) * sum(y - y_p)\n",
        "  b1 = b1 - (l*d1)\n",
        "  b0 = b0 - (l*d0)\n",
        "\n",
        "print(b1,b0)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.08676120153957817 0.16056812487506927\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qThqrG1ykD8c",
        "colab_type": "text"
      },
      "source": [
        "**Logistic regression using L1 regualrization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMCR5gB7kKTq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "50a971e5-2ba7-41b9-ca51-1cd44fb4edad"
      },
      "source": [
        "X1 = df1.iloc[:,0:4].values\n",
        "y1 = df1.iloc[:,4].values\n",
        "lam = 0.1\n",
        "def sigmoid(Z):\n",
        "  return 1 /(1+np.exp(-Z))\n",
        "\n",
        "def loss(y1,y_hat):\n",
        "  return -np.mean(y1*np.log(y_hat) + (1-y1)*(np.log(1-y_hat))) + (lam * (np.sum(W)))\n",
        "\n",
        "W = np.zeros((4,1))\n",
        "b = np.zeros((1,1))\n",
        "\n",
        "m = len(y1)\n",
        "lr = 0.001\n",
        "for epoch in range(1000):\n",
        "  Z = np.matmul(X1,W)+b\n",
        "  A = sigmoid(Z)\n",
        "  logistic_loss = loss(y1,A)\n",
        "  dz = A - y1\n",
        "  dw = 1/m * np.matmul(X1.T,dz) + lam\n",
        "  db = np.sum(dz)\n",
        "\n",
        "  W = W - lr*dw\n",
        "  b = b - lr*db\n",
        "\n",
        "  if epoch % 100 == 0:\n",
        "    print(logistic_loss)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6931471805599453\n",
            "0.07290611272185021\n",
            "-0.3202022413463126\n",
            "-0.7082948401294828\n",
            "-1.0914563943592128\n",
            "-1.4697713184994101\n",
            "-1.8433236049289188\n",
            "-2.2121967091352186\n",
            "-2.57647344545215\n",
            "-2.93623589285183\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DirVZbnhkOhy",
        "colab_type": "text"
      },
      "source": [
        "**Logistic regression using L2 regualrization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJHzlZOzkRq3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "5736b576-4032-4257-cde4-acbaada11669"
      },
      "source": [
        "X1 = df1.iloc[:,0:4].values\n",
        "y1 = df1.iloc[:,4].values\n",
        "lam = 0.1\n",
        "def sigmoid(Z):\n",
        "  return 1 /(1+np.exp(-Z))\n",
        "\n",
        "def loss(y1,y_hat):\n",
        "  return -np.mean(y1*np.log(y_hat) + (1-y1)*(np.log(1-y_hat))) + (lam * (np.sum(np.square(W))))\n",
        "\n",
        "W = np.zeros((4,1))\n",
        "b = np.zeros((1,1))\n",
        "\n",
        "m = len(y1)\n",
        "lr = 0.001\n",
        "for epoch in range(1000):\n",
        "  Z = np.matmul(X1,W)+b\n",
        "  A = sigmoid(Z)\n",
        "  logistic_loss = loss(y1,A)\n",
        "  dz = A - y1\n",
        "  dw = 1/m * np.matmul(X1.T,dz) + lam * W\n",
        "  db = np.sum(dz)\n",
        "\n",
        "  W = W - lr*dw\n",
        "  b = b - lr*db\n",
        "\n",
        "  if epoch % 100 == 0:\n",
        "    print(logistic_loss)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6931471805599453\n",
            "0.47138981916897604\n",
            "0.4728356830648328\n",
            "0.4756240336199851\n",
            "0.479653612646063\n",
            "0.4848287008017039\n",
            "0.49105886821553657\n",
            "0.49825873421116\n",
            "0.5063477358755237\n",
            "0.5152499052094035\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHEtUriwkZf7",
        "colab_type": "text"
      },
      "source": [
        "**K Means Clustering Algorithm**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmXqI1o2kc9n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class K_Means:\n",
        "    def __init__(self, k=2, tol=0.001, max_iter=300):\n",
        "        self.k = k\n",
        "        self.tol = tol\n",
        "        self.max_iter = max_iter\n",
        "\n",
        "    def fit(self,data):\n",
        "\n",
        "        self.centroids = {}\n",
        "\n",
        "        for i in range(self.k):\n",
        "            self.centroids[i] = data[i]\n",
        "\n",
        "        for i in range(self.max_iter):\n",
        "            self.classifications = {}\n",
        "\n",
        "            for i in range(self.k):\n",
        "                self.classifications[i] = []\n",
        "\n",
        "            for featureset in X:\n",
        "                distances = [np.linalg.norm(featureset-self.centroids[centroid]) for centroid in self.centroids]\n",
        "                classification = distances.index(min(distances))\n",
        "                self.classifications[classification].append(featureset)\n",
        "\n",
        "            prev_centroids = dict(self.centroids)\n",
        "\n",
        "            for classification in self.classifications:\n",
        "                self.centroids[classification] = np.average(self.classifications[classification],axis=0)\n",
        "\n",
        "            optimized = True\n",
        "\n",
        "            for c in self.centroids:\n",
        "                original_centroid = prev_centroids[c]\n",
        "                current_centroid = self.centroids[c]\n",
        "                if np.sum((current_centroid-original_centroid)/original_centroid*100.0) > self.tol:\n",
        "                    print(np.sum((current_centroid-original_centroid)/original_centroid*100.0))\n",
        "                    optimized = False\n",
        "\n",
        "            if optimized:\n",
        "                break\n",
        "\n",
        "    def predict(self,data):\n",
        "        distances = [np.linalg.norm(data-self.centroids[centroid]) for centroid in self.centroids]\n",
        "        classification = distances.index(min(distances))\n",
        "        return classification\n",
        "        \n",
        "colors = 10*[\"g\",\"r\",\"c\",\"b\",\"k\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xA1xWpakjda",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = df3.iloc[:,0:2].values\n",
        "clf = K_Means()\n",
        "clf.fit(X)\n",
        "\n",
        "for centroid in clf.centroids:\n",
        "    plt.scatter(clf.centroids[centroid][0], clf.centroids[centroid][1],\n",
        "                marker=\"o\", color=\"k\", s=150, linewidths=5)\n",
        "\n",
        "for classification in clf.classifications:\n",
        "    color = colors[classification]\n",
        "    for featureset in clf.classifications[classification]:\n",
        "        plt.scatter(featureset[0], featureset[1], marker=\"x\", color=color, s=150, linewidths=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fkMXzlVkkd2",
        "colab_type": "text"
      },
      "source": [
        "**Problem Statement 4**\n",
        "\n",
        "**Linear Regression from scratch using OOPS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z40aaLjAkpmU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class LinearRegressionModel():\n",
        "\n",
        "    def __init__(self, dataset, learning_rate, num_iterations):\n",
        "        self.dataset = np.array(dataset)\n",
        "        self.b = 0  \n",
        "        self.m = 0  \n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_iterations = num_iterations\n",
        "        self.M = len(self.dataset)\n",
        "        self.total_error = 0\n",
        "\n",
        "    def apply_gradient_descent(self):\n",
        "        for i in range(self.num_iterations):\n",
        "            self.do_gradient_step()\n",
        "\n",
        "    def do_gradient_step(self):\n",
        "        b_summation = 0\n",
        "        m_summation = 0\n",
        "        for i in range(self.M):\n",
        "            x_value = self.dataset[i, 0]\n",
        "            y_value = self.dataset[i, 1]\n",
        "            b_summation += (((self.m * x_value) + self.b) - y_value) \n",
        "            m_summation += (((self.m * x_value) + self.b) - y_value) * x_value\n",
        "        self.b = self.b - (self.learning_rate * (1/self.M) * b_summation)\n",
        "        self.m = self.m - (self.learning_rate * (1/self.M) * m_summation)\n",
        "      \n",
        "    def compute_error(self):\n",
        "        for i in range(self.M):\n",
        "            x_value = self.dataset[i, 0]\n",
        "            y_value = self.dataset[i, 1]\n",
        "            self.total_error += ((self.m * x_value) + self.b) - y_value\n",
        "        return self.total_error\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"Results: b: {}, m: {}, Final Total error: {}\".format(round(self.b, 2), round(self.m, 2), round(self.compute_error(), 2))\n",
        "\n",
        "    def get_prediction_based_on(self, x):\n",
        "        return round(float((self.m * x) + self.b), 2) # Type: Numpy float.\n",
        "\n",
        "def main():\n",
        "    school_dataset = np.genfromtxt(DATASET_PATH, delimiter=\",\")\n",
        "    lr = LinearRegressionModel(school_dataset, 0.0001, 1000)\n",
        "    lr.apply_gradient_descent()\n",
        "    hours = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
        "    for hour in hours:\n",
        "        print(\"Studied {} hours and got {} points.\".format(hour, lr.get_prediction_based_on(hour)))\n",
        "    print(lr)\n",
        "\n",
        "if __name__ == \"__main__\": main()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsgTSo_ok_T_",
        "colab_type": "text"
      },
      "source": [
        "**Logistic Regression from scratch using OOPS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hX1cBSzXlJcJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LogisticRegression:\n",
        "  def __init__(self, learning_rate, num_iters, fit_intercept = True, verbose = False):\n",
        "    self.learning_rate = learning_rate\n",
        "    self.num_iters = num_iters\n",
        "    self.fit_intercept = fit_intercept\n",
        "    self.verbose = verbose\n",
        "  def __add_intercept(self, X):\n",
        "    intercept = np.ones((X.shape[0],1))\n",
        "    return np.concatenate((intercept,X),axis=1)\n",
        "  def __sigmoid(self,z):\n",
        "    return 1/(1+np.exp(-z))\n",
        "  def __loss(self, h, y):\n",
        "    return (-y * np.log(h) - (1-y) * np.log(1-h)).mean()\n",
        "  \n",
        "  def fit(self,X,y):\n",
        "    if self.fit_intercept:\n",
        "      X = self.__add_intercept(X)\n",
        "    self.theta = np.zeros(X.shape[1])\n",
        "    \n",
        "    for i in range(self.num_iters):\n",
        "      z = np.dot(X,self.theta)\n",
        "      h = self.__sigmoid(z)\n",
        "      gradient = np.dot(X.T,(h-y))/y.size\n",
        "      \n",
        "      self.theta -= self.learning_rate * gradient\n",
        "      \n",
        "      z = np.dot(X,self.theta)\n",
        "      h = self.__sigmoid(z)\n",
        "      loss = self.__loss(h,y)\n",
        "      \n",
        "      if self.verbose == True and i % 1000 == 0:\n",
        "        print(f'Loss: {loss}\\t')\n",
        "  def predict_probability(self,X):\n",
        "    if self.fit_intercept:\n",
        "      X = self.__add_intercept(X)\n",
        "    return self.__sigmoid(np.dot(X,self.theta))\n",
        "  def predict(self,X):\n",
        "    return (self.predict_probability(X).round())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKbItH9VlP-L",
        "colab_type": "text"
      },
      "source": [
        "**K Means from scratch using OOPS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4lTGtVflRM_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class K_Means:\n",
        "    def __init__(self, k=2, tol=0.001, max_iter=300):\n",
        "        self.k = k\n",
        "        self.tol = tol\n",
        "        self.max_iter = max_iter\n",
        "\n",
        "    def fit(self,data):\n",
        "\n",
        "        self.centroids = {}\n",
        "\n",
        "        for i in range(self.k):\n",
        "            self.centroids[i] = data[i]\n",
        "\n",
        "        for i in range(self.max_iter):\n",
        "            self.classifications = {}\n",
        "\n",
        "            for i in range(self.k):\n",
        "                self.classifications[i] = []\n",
        "\n",
        "            for featureset in X:\n",
        "                distances = [np.linalg.norm(featureset-self.centroids[centroid]) for centroid in self.centroids]\n",
        "                classification = distances.index(min(distances))\n",
        "                self.classifications[classification].append(featureset)\n",
        "\n",
        "            prev_centroids = dict(self.centroids)\n",
        "\n",
        "            for classification in self.classifications:\n",
        "                self.centroids[classification] = np.average(self.classifications[classification],axis=0)\n",
        "\n",
        "            optimized = True\n",
        "\n",
        "            for c in self.centroids:\n",
        "                original_centroid = prev_centroids[c]\n",
        "                current_centroid = self.centroids[c]\n",
        "                if np.sum((current_centroid-original_centroid)/original_centroid*100.0) > self.tol:\n",
        "                    print(np.sum((current_centroid-original_centroid)/original_centroid*100.0))\n",
        "                    optimized = False\n",
        "\n",
        "            if optimized:\n",
        "                break\n",
        "\n",
        "    def predict(self,data):\n",
        "        distances = [np.linalg.norm(data-self.centroids[centroid]) for centroid in self.centroids]\n",
        "        classification = distances.index(min(distances))\n",
        "        return classification\n",
        "        \n",
        "colors = 10*[\"g\",\"r\",\"c\",\"b\",\"k\"]"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}